<div class="col-sm-8 text-left" style="text-align: justify;">  

  <section id="goal" class="goal">
    <div style="text-align: justify;">
      <h2>Conversational Health Interfaces @CUI2025</h2>
    </div>

    <p>This workshop aims to foster a dialogue to explore these challenges and opportunities, discussing how to optimize the beneficial impacts of LLM-powered CUIs on health and wellbeing while effectively managing and mitigating associated risks. Participants will engage in examining themes such as privacy-centric conversational interventions, proactive and adaptive strategies and agency which are pivotal in designing CUIs that are not only effective but also ethically sound and user-friendly. These discussions will help in developing frameworks and collaborations that will guide the ethical development and practical application of CUIs in health and wellness, ensuring they enhance rather than compromise user wellbeing.</p>
  </section>

  <hr>

  <section id="themes" class="themes">
    <h3>Workshop Themes</h3>
    <ul>
      <li><strong>Structuring Conversations</strong> &mdash; Design conversational interactions that respect situational context.</li> 
      <li><strong>Privacy and Trust Concerns</strong> &mdash; Address privacy issues in CUI, focusing on discreet and secure interactions.</li> 
      <li><strong>User Groups and Personalization</strong> &mdash; Tailor CUI to diverse user groups, enhancing personalization and relevance.</li>
      <li><strong>Proactive Interventions</strong> &mdash; Design CUIs that proactively support health and wellness based on user behavior.</li> 
      <li><strong>User Agency</strong> &mdash; Enhance user control and agency in interactions with CUI.</li>
    </ul>
    <hr>
  </section>

  <section id="call" class="call">
    <h3>Call for Participation</h3>
    <p>We invite researchers, practitioners, and enthusiasts from various disciplines to share their insights and contribute to the synthesis of knowledge in this area of CUI and health and wellbeing. While formal paper submissions are welcomed, they are not mandatory for participation.</p>

    <p>Attendees may choose to engage by submitting:</p>

    <p><b>Position papers or short research papers</b> (up to 4 pages, following the <a href="https://www.acm.org/publications/proceedings-template" target="_blank" rel="noopener noreferrer">ACM Extended Abstract format</a>) detailing studies, novel systems, new theories, or ongoing challenges in the field.</p>

    <p><b>Short expressions of interest</b> that outline their background and interest in the workshop themes. These can be submitted via email to the organizers and should include a brief description of the applicant's relevant experience and a link to their professional or scholarly webpage.</p>

    <p><b>Paper presentations</b> &mdash; If you‚Äôve published work related to conversational agents and healthcare, we‚Äôd love to include it in our session!</p>

    <p>Submit your papers as a single PDF via email to <b>shashank.ahire@hci.uni-hannover.de</b></p>
    <hr>    
  </section>

  <section id="dates" class="dates">
    <h3>Key Dates</h3>
    <ul>
      <li><strong><s>May 16, 2025</s> June 3, 2025:</strong> Submission deadline</li>
      <li><strong>June 7, 2025:</strong> Acceptance notification [will be adjusted based on early-bird conference registration due]</li>
      <li><strong>June 20, 2025:</strong> Camera-ready deadline</li>
      <li><strong>July 08, 2025:</strong> Workshop day</li>
    </ul>
  </section>

  <hr> 

  <section id="organizers" class="organizers">
    <h3>Organizers</h3>

    <p><a href="https://shashankahire.com" target="_blank" rel="noopener noreferrer">Shashank Ahire</a> is a PhD candidate in the Human-Computer Interaction group at Leibniz University Hannover. His research focuses on developing proactive voice interventions for the health and wellbeing of knowledge workers.</p>

    <p><a href="https://www.linkedin.com/in/melissaguyre" target="_blank" rel="noopener noreferrer">Melissa Guyre</a> is a Product Management Lead at Panasonic Well, focusing on AI-driven family wellness product incubation, including the development of a conversational AI Family Wellness Coach.</p>

    <p><a href="https://hci.cs.umanitoba.ca/people/bio/bradley-rey" target="_blank" rel="noopener noreferrer">Bradley Rey</a> is an Assistant Professor at the University of Winnipeg in the Department of Applied Computer Science. His research focuses on designing and developing in-situ wearable interfaces that empower people to better explore and make sense of their personal health data anytime and anywhere.</p>

    <p><a href="https://minha-lee.github.io/" target="_blank" rel="noopener noreferrer">Minha Lee</a> is an Assistant Professor at the Eindhoven University of Technology in the Department of Industrial Design, with a background in philosophy, digital arts, and HCI. Her research concerns morally relevant interactions with various agents like robots or chatbots, exploring moral concepts like compassion and trust.</p>
 
    <p><a href="https://research.ibm.com/people/heloisa-caroline-de-souza-pereira-candello-de-souza-pereira-candello" target="_blank" rel="noopener noreferrer">Heloisa Candello</a> is a research scientist and manager of the Human-centered and Responsible Technologies group at IBM Research. Her work focuses on human and social aspects of Artificial Intelligence systems, particularly CUI.</p>
  </section>

  <hr> 

  <section id="paper" class="paper">
    <h3>Workshop Paper</h3>

    <p><strong>Title &mdash;</strong> Focus on the Successes: The Conflict between the Goals of ‚ÄúData Science‚Äù and What Patients Want from Conversational Robots in Healthcare</p>
    <p><strong>Authors &mdash;</strong> Casey C. Bennett</p>
    <p><a href="Bennett_ ConversationalRobots_Healthcare_Conflict_Ver2 _Final.pdf" target="_blank" rel="noopener noreferrer">üìÑ Download Paper</a></p>

    <p><strong>Title &mdash;</strong> Passive At-Home Health Monitoring Systems for Older Adult Care</p>
    <p><strong>Authors &mdash;</strong> Elaine Czech, Aisling Ann O‚ÄôKane, Kenton O‚ÄôHara, Eleni Margariti, Abigail Durrant, David Kirk, and Ian Craddock</p>
    <p><a href="Technology_Mediated_Caregiving_for_Older_Adults_Aging_in_Place.pdf" target="_blank" rel="noopener noreferrer">üìÑ Download Paper</a></p>
  </section>

  <hr>

  <!-- Integrated Workshop Report / Blogpost -->
  <section id="workshop-report" class="workshop-report">
    <h3>CUI 2025 Workshop Report &amp; Insights</h3>

    <p>At CUI 2025, our workshop <strong>‚ÄúConversational Health Interfaces in the Era of LLMs: Designing for Engagement, Privacy, and Wellbeing‚Äù</strong> brought together researchers, designers, and practitioners to explore the promises and pitfalls of using conversational user interfaces (CUIs) for health and wellbeing. Across group activities, we unpacked challenges around bias and fairness, user agency in stress interventions, and proactivity in exercise support.</p>

    <h4>Biases and Fairness in CUIs</h4>
    <p>One recurring theme was the bias embedded in foundation models and how it manifests in health conversations.</p>
    <ul>
      <li><strong>Benevolent bias isn‚Äôt always benevolent:</strong> When agents framed responses positively (‚ÄúYou‚Äôve achieved a lot at such a young age‚Äù), participants perceived them as patronizing or insincere. Forced positivity often risks becoming toxic positivity.</li>
      <li><strong>Empathy vs. anthropomorphism:</strong> Overly empathetic or human-like responses sometimes triggered negative reactions (an ‚Äúuncanny valley‚Äù effect). Context matters‚Äîtransactional health advice differs from emotional support.</li>
      <li><strong>Transparency matters:</strong> Users want to know how their history shapes responses. CUIs should clearly explain why certain suggestions are made.</li>
      <li><strong>Safeguarding vs. misrepresentation:</strong> Attempts to make models ‚Äúsafer‚Äù can sometimes distort representation, making outputs seem biased in different ways.</li>
    </ul>
    <p><em>Open research questions:</em></p>
    <ul>
      <li>How can we study bias and fairness ethically without harming participants?</li>
      <li>What triggers cause a chatbot to appear biased or unfair?</li>
      <li>How much does ‚Äúpersonality‚Äù matter in user perceptions of fairness?</li>
    </ul>

    <h4>Stress Interventions and User Agency</h4>
    <p>Another group examined how CUIs might deliver stress interventions in office settings‚Äîa context where privacy and discretion are paramount.</p>
    <ul>
      <li><strong>Stress detection is tricky:</strong> Sensors may misinterpret energy as stress. Users should confirm or adjust predictions rather than being told ‚Äúyou are stressed.‚Äù</li>
      <li><strong>Delivery must respect context:</strong> A desk worker in a meeting might want to defer an intervention. CUIs should ask ‚ÄúWould you like to do this now or later?‚Äù rather than forcing the moment.</li>
      <li><strong>Multi-modality is essential:</strong> Voice might be intrusive in shared spaces; offering text or silent modes increases accessibility.</li>
      <li><strong>Customization at onboarding:</strong> Let users pre-set preferences (e.g., how to handle stress during meetings) and refine them post-intervention, rather than mid-stress.</li>
      <li><strong>Transparency and trust:</strong> Users need to know where data comes from, who has access, and whether employers or colleagues can see it. Agency comes from informed choice.</li>
    </ul>
    <p><em>Takeaway:</em> stress-support CUIs must empower users with control over when, how, and why interventions occur.</p>

    <h4>Proactivity in Exercise Support</h4>
    <p>The third group explored proactive CUIs for exercise, asking how interventions can be encouraging without being intrusive.</p>
    <ul>
      <li><strong>The fine line between support and overbearing:</strong> Enthusiastic prompts may motivate some users, but alienate others who feel patronized.</li>
      <li><strong>Context changes everything:</strong> Illness, mood, location, or long-term fitness goals all affect what kind of support is appropriate.</li>
      <li><strong>Beyond metrics:</strong> Moving from rigid, goal-based prompts to reflective dialogues (‚ÄúHow are you feeling right now?‚Äù) can create more meaningful, long-term engagement.</li>
      <li><strong>Ethical concerns:</strong> Proactive CUIs risk fostering over-dependence or ‚Äúaddiction‚Äù to optimization. How much is too much intervention?</li>
    </ul>
    <p>This discussion highlighted the importance of designing proactive CUIs that listen, adapt, and evolve with users‚Äîbalancing helpfulness with respect for autonomy.</p>

    <h4>Closing Thoughts</h4>
    <p>Across these sessions, one message was clear: Conversational Health Interfaces must walk a delicate line. They need to be proactive without being intrusive, empathetic without being condescending, and supportive without compromising user agency or privacy.</p>
    <p>To get there, we must:</p>
    <ul>
      <li>Build transparency into every interaction</li>
      <li>Offer flexible modalities for diverse contexts</li>
      <li>Study bias with ethical, innovative methods</li>
      <li>Design with the lived experiences of users at the center</li>
    </ul>
    <p>The workshop was only the beginning, but it underscored that the future of health CUIs depends not just on smarter models‚Äî<strong>but on smarter design choices that empower people to take charge of their health and wellbeing.</strong></p>
  </section>

</div>
